# -*- coding: utf-8 -*-
"""Drift_detection_Task2_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xAl2cEBJF09IVSExwbjFVAJ7HcVBAPS5
"""

import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_curve, auc
from sklearn.decomposition import TruncatedSVD
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier
from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, recall_score
import os
import pm4py
from pm4py.objects.log.util import dataframe_utils
from pm4py.objects.conversion.log import converter as log_converter
from pm4py.algo.discovery.dfg import algorithm as dfg_discovery
from pm4py.visualization.dfg import visualizer as dfg_visualization
from pm4py.algo.discovery.alpha import algorithm as alpha_miner
from pm4py.visualization.petri_net import visualizer as pn_visualizer
from pm4py.algo.filtering.log.timestamp import timestamp_filter
from pm4py.algo.conformance.tokenreplay import algorithm as token_replay
from pm4py.algo.evaluation.replay_fitness import algorithm as replay_fitness
from pm4py.algo.discovery.heuristics import algorithm as heuristics_miner
from pm4py.visualization.heuristics_net import visualizer as hn_visualizer

# Load and preprocess data
file_path = 'BPI2016_Questions.csv'
data = pd.read_csv(file_path, delimiter=';', encoding='latin1')
data['ContactDate'] = pd.to_datetime(data['ContactDate'], format='%Y-%m-%d')
data['ContactTimeStart'] = pd.to_datetime(data['ContactTimeStart'], format='%H:%M:%S.%f').dt.time
data['ContactTimeEnd'] = pd.to_datetime(data['ContactTimeEnd'], format='%H:%M:%S.%f').dt.time

# Check for duplicates and missing values
duplicates = data.duplicated().sum()
missing_values = data.isnull().sum()
print("Number of duplicate rows:", duplicates)
print("Missing values per column:")
print(missing_values)

# Create YearMonth column and group data
data['YearMonth'] = data['ContactDate'].dt.to_period('M')
monthly_segments = data.groupby('YearMonth')
segment_sizes = monthly_segments.size()
print("\nSegment sizes (first few):")
print(segment_sizes.head())

# Plot top 5 question categories over time
grouped_data = data.groupby(['YearMonth', 'QuestionTopic_EN']).size().unstack(fill_value=0)
top_categories = grouped_data.sum().nlargest(5).index

plt.figure(figsize=(12, 6))
for category in top_categories:
    plt.plot(grouped_data.index.astype(str), grouped_data[category], label=category, marker='o')

plt.title('Top 5 Question Categories Over Time')
plt.xlabel('Month')
plt.ylabel('Number of Questions')
plt.legend(title='Question Category', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.xticks(rotation=45)
plt.tight_layout()
plt.grid(True, linestyle='--', alpha=0.7)
plt.show()

# Detect drift using CUSUM
def add_drift_column(data):
    initial_mean = data['QuestionThemeID'].mean()
    initial_std = data['QuestionThemeID'].std()
    k = initial_std / 2
    h = 5 * initial_std

    cusum_list = []
    previous_cusum = 0
    for xt in data['QuestionThemeID']:
        current_cusum = max(0, previous_cusum + (xt - initial_mean - k))
        cusum_list.append(current_cusum)
        previous_cusum = current_cusum

    data['CUSUM'] = cusum_list
    data['Drift'] = data['CUSUM'] > h
    return data, h

data, h = add_drift_column(data)

# Plot CUSUM chart
plt.figure(figsize=(14, 7))
plt.plot(data['ContactDate'], data['CUSUM'], label='CUSUM', color='blue')
plt.axhline(y=h, color='red', linestyle='--', label='Threshold (h)')
drift_points = data[data['Drift']]
plt.scatter(drift_points['ContactDate'], drift_points['CUSUM'], color='red', label='Potential Drift Points')
plt.xlabel('Date')
plt.ylabel('CUSUM')
plt.title('CUSUM Analysis for Drift Detection')
plt.legend()
plt.show()

# Plot CUSUM chart
plt.figure(figsize=(14, 7))
#plt.plot(data['ContactDate'], data['CUSUM'], label='CUSUM', color='blue')
plt.axhline(y=h, color='red', linestyle='--', label='Threshold (h)')
drift_points = data[data['Drift']]
plt.scatter(drift_points['ContactDate'], drift_points['CUSUM'], color='red', label='Potential Drift Points')
plt.xlabel('Date')
plt.ylabel('CUSUM')
plt.title('CUSUM Analysis for Drift Detection')
plt.legend()
plt.show()

# Step 3: Perform ing Machin Learning Bead Localization using Ensemble Method

def calculate_metrics(y_true, y_pred):
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    return accuracy, precision, recall

def calculate_cusum_metrics(data):
    y_true = data['Drift']
    y_scores = data['CUSUM']
    y_pred = (y_scores > y_scores.mean()).astype(int)
    fpr, tpr, _ = roc_curve(y_true, y_scores)
    roc_auc = auc(fpr, tpr)
    metrics = calculate_metrics(y_true, y_pred)
    return metrics, (fpr, tpr, roc_auc)

def ensemble_with_svd(data, n_components=2, n_estimators=100):
    features = data[['QuestionThemeID', 'QuestionSubthemeID', 'QuestionTopicID']]
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)

    svd = TruncatedSVD(n_components=n_components, random_state=42)
    features_svd = svd.fit_transform(features_scaled)

    rf = RandomForestClassifier(n_estimators=n_estimators, random_state=42)
    ensemble = BaggingClassifier(base_estimator=rf, n_estimators=10, random_state=42)

    ensemble.fit(features_svd, data['Drift'])

    y_scores = ensemble.predict_proba(features_svd)[:, 1]
    y_pred = ensemble.predict(features_svd)

    fpr, tpr, _ = roc_curve(data['Drift'], y_scores)
    roc_auc = auc(fpr, tpr)

    metrics = calculate_metrics(data['Drift'], y_pred)
    return metrics, (fpr, tpr, roc_auc)

# Calculate metrics and ROC curves
cusum_metrics, cusum_roc = calculate_cusum_metrics(data)
ensemble_metrics, ensemble_roc = calculate_ensemble_metrics_with_svd(data)

# Plot ROC curves separately
def plot_roc_curve(fpr, tpr, roc_auc, method_name, color):
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color=color, lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'{method_name} ROC Curve')
    plt.legend(loc="lower right")
    plt.savefig(f'SVG/ROC_Curve_{method_name.replace(" ", "_")}.eps', format='eps', dpi=300, bbox_inches='tight')
    plt.show()

# Plot ROC curves
plot_roc_curve(cusum_roc[0], cusum_roc[1], cusum_roc[2], 'CUSUM', 'blue')
plot_roc_curve(ensemble_roc[0], ensemble_roc[1], ensemble_roc[2], 'Ensemble', 'red')

# Step 4: Process Mining Analysis
# Convert to event log
df = dataframe_utils.convert_timestamp_columns_in_df(data)
df['case:concept:name'] = df['CustomerID']
df['concept:name'] = df['QuestionTheme_EN']
df['time:timestamp'] = df['ContactDate']
event_log = log_converter.apply(df)

# Discover DFG
dfg = dfg_discovery.apply(event_log)

# Visualize DFG for display
display_parameters = {dfg_visualization.Variants.FREQUENCY.value.Parameters.FORMAT: "png"}
gviz_display = dfg_visualization.apply(dfg, log=event_log, variant=dfg_visualization.Variants.FREQUENCY, parameters=display_parameters)

# Display the graph
dfg_visualization.view(gviz_display)

# Visualize DFG for EPS export
export_parameters = {dfg_visualization.Variants.FREQUENCY.value.Parameters.FORMAT: "dot"}
gviz_export = dfg_visualization.apply(dfg, log=event_log, variant=dfg_visualization.Variants.FREQUENCY, parameters=export_parameters)

# Save as DOT file
dot_file = "SVG/dfg_graph.dot"
dfg_visualization.save(gviz_export, dot_file)

# Convert DOT to EPS using Graphviz
eps_file = "SVG/dfg_graph.eps"
os.system(f"dot -Teps {dot_file} -o {eps_file}")

print(f"DFG saved as EPS: {eps_file}")

# Optional: Remove the intermediate DOT file
os.remove(dot_file)

# Filter logs based on timestamp
start_date_pre_drift = '2015-07-01'
end_date_pre_drift = '2015-08-31'
pre_drift_event_log = timestamp_filter.filter_traces_contained(event_log, pd.Timestamp(start_date_pre_drift), pd.Timestamp(end_date_pre_drift))

start_date_post_drift = '2015-09-01'
end_date_post_drift = '2015-10-31'
post_drift_event_log = timestamp_filter.filter_traces_contained(event_log, pd.Timestamp(start_date_post_drift), pd.Timestamp(end_date_post_drift))

# Discover process models
net_pre, initial_marking_pre, final_marking_pre = alpha_miner.apply(pre_drift_event_log)
net_post, initial_marking_post, final_marking_post = alpha_miner.apply(post_drift_event_log)

# Function to visualize and save process model
def visualize_and_save_model(net, initial_marking, final_marking, filename_prefix):
    # Visualize for display
    gviz_display = pn_visualizer.apply(net, initial_marking, final_marking, parameters={"format": "png"})
    pn_visualizer.view(gviz_display)

    # Visualize for EPS export
    gviz_export = pn_visualizer.apply(net, initial_marking, final_marking, parameters={"format": "dot"})

    # Save as DOT file
    dot_file = f"{filename_prefix}_model.dot"
    pn_visualizer.save(gviz_export, dot_file)

    # Convert DOT to EPS using Graphviz
    eps_file = f"{filename_prefix}_model.eps"
    os.system(f"dot -Teps {dot_file} -o {eps_file}")

    print(f"Process model saved as EPS: {eps_file}")

    # Optional: Remove the intermediate DOT file
    os.remove(dot_file)

# Visualize and save pre-drift model
visualize_and_save_model(net_pre, initial_marking_pre, final_marking_pre, "pre_drift")

# Visualize and save post-drift model
visualize_and_save_model(net_post, initial_marking_post, final_marking_post, "post_drift")

# Calculate replay fitness
replay_result_pre = token_replay.apply(pre_drift_event_log, net_pre, initial_marking_pre, final_marking_pre)
pre_drift_fitness = replay_fitness.evaluate(replay_result_pre)

replay_result_post = token_replay.apply(post_drift_event_log, net_post, initial_marking_post, final_marking_post)
post_drift_fitness = replay_fitness.evaluate(replay_result_post)

print("Pre-drift fitness:", pre_drift_fitness)
print("Post-drift fitness:", post_drift_fitness)

# Calculate transition probabilities
pre_drift_ts = heuristics_miner.apply_heu(pre_drift_event_log)
post_drift_ts = heuristics_miner.apply_heu(post_drift_event_log)

# Function to visualize and save transition probabilities
def visualize_and_save_ts(ts, filename_prefix):
    # Visualize for display
    gviz_display = hn_visualizer.apply(ts, parameters={"format": "png"})
    hn_visualizer.view(gviz_display)

    # Visualize for EPS export
    gviz_export = hn_visualizer.apply(ts, parameters={"format": "dot"})

    # Save as DOT file
    dot_file = f"{filename_prefix}_ts.dot"
    hn_visualizer.save(gviz_export, dot_file)

    # Convert DOT to EPS using Graphviz
    eps_file = f"{filename_prefix}_ts.eps"
    os.system(f"dot -Teps {dot_file} -o {eps_file}")

    print(f"Transition probabilities saved as EPS: {eps_file}")

    # Optional: Remove the intermediate DOT file
    os.remove(dot_file)

# Visualize and save pre-drift transition probabilities
visualize_and_save_ts(pre_drift_ts, "pre_drift")

# Visualize and save post-drift transition probabilities
visualize_and_save_ts(post_drift_ts, "post_drift")

def calculate_alpha_miner_metrics_with_svd(data, n_components=2):
    features = data[['QuestionThemeID', 'QuestionSubthemeID', 'QuestionTopicID']]
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)

    svd = TruncatedSVD(n_components=n_components, random_state=42)
    features_svd = svd.fit_transform(features_scaled)

    y_scores = features_svd[:, 0]
    y_pred = (y_scores > y_scores.mean()).astype(int)

    fpr, tpr, _ = roc_curve(data['Drift'], y_scores)
    roc_auc = auc(fpr, tpr)

    metrics = calculate_metrics(data['Drift'], y_pred)
    return metrics, (fpr, tpr, roc_auc)


# Calculate metrics and ROC curves
alpha_metrics, alpha_roc = calculate_alpha_miner_metrics_with_svd(data)

# Print results
methods = ['CUSUM', 'Ensemble (RF + Bagging) with SVD', 'Alpha Miner Proxy with SVD']
metrics_names = ['Accuracy', 'Precision', 'Recall']
print("Performance Metrics:")
print("-------------------")
for method, result in zip(methods, [cusum_metrics, ensemble_metrics, alpha_metrics]):
    print(f"\n{method}:")
    for metric, value in zip(metrics_names, result):
        print(f"{metric}: {value:.4f}")

# Plot ROC curves
plot_roc_curve(alpha_roc[0], alpha_roc[1], alpha_roc[2], 'Alpha Miner Proxy', 'green')

# Plot combined ROC curves
plt.figure(figsize=(10, 8))
plt.plot(cusum_roc[0], cusum_roc[1], color='blue', lw=2, label=f'CUSUM (AUC = {cusum_roc[2]:.2f})')
plt.plot(ensemble_roc[0], ensemble_roc[1], color='red', lw=2, label=f'Ensemble (RF + Bagging) with SVD (AUC = {ensemble_roc[2]:.2f})')
plt.plot(alpha_roc[0], alpha_roc[1], color='green', lw=2, label=f'Alpha Miner Proxy with SVD (AUC = {alpha_roc[2]:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Combined Receiver Operating Characteristic (ROC) Curves')
plt.legend(loc="lower right")
plt.savefig('SVG/ROC_Curves_Combined.eps', format='eps', dpi=300, bbox_inches='tight')
plt.show()

# Visualize performance metrics
fig, ax = plt.subplots(figsize=(12, 6))

x = np.arange(len(metrics_names))
width = 0.25

rects1 = ax.bar(x - width, cusum_metrics, width, label='CUSUM')
rects2 = ax.bar(x, ensemble_metrics, width, label='Ensemble (RF + Bagging) with SVD')
rects3 = ax.bar(x + width, alpha_metrics, width, label='Alpha Miner Proxy with SVD')

ax.set_ylabel('Scores')
ax.set_title('Performance Metrics Comparison')
ax.set_xticks(x)
ax.set_xticklabels(metrics_names)
ax.legend()

plt.tight_layout()
plt.savefig('SVG/Performance_Metrics_Comparison_rfbc.eps', format='eps', dpi=300, bbox_inches='tight')
plt.show()

