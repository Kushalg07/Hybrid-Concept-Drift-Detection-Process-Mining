# -*- coding: utf-8 -*-
"""Drift_detection_Task2_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NRZ1BleM8HFs1-eEvxptglVRP8hzlIaP
"""

import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import roc_curve, auc
import os
import pm4py
from pm4py.objects.log.util import dataframe_utils
from pm4py.objects.conversion.log import converter as log_converter
from pm4py.algo.discovery.dfg import algorithm as dfg_discovery
from pm4py.visualization.dfg import visualizer as dfg_visualization
from pm4py.algo.discovery.alpha import algorithm as alpha_miner
from pm4py.visualization.petri_net import visualizer as pn_visualizer
from pm4py.algo.filtering.log.timestamp import timestamp_filter
from pm4py.algo.conformance.tokenreplay import algorithm as token_replay
from pm4py.algo.evaluation.replay_fitness import algorithm as replay_fitness
from pm4py.algo.discovery.heuristics import algorithm as heuristics_miner
from pm4py.visualization.heuristics_net import visualizer as hn_visualizer

# Step 1: Load and preprocess data
file_path = 'BPI2016_Questions.csv'
data = pd.read_csv(file_path, delimiter=';', encoding='latin1')
data['ContactDate'] = pd.to_datetime(data['ContactDate'], format='%Y-%m-%d')
data['ContactTimeStart'] = pd.to_datetime(data['ContactTimeStart'], format='%H:%M:%S.%f').dt.time
data['ContactTimeEnd'] = pd.to_datetime(data['ContactTimeEnd'], format='%H:%M:%S.%f').dt.time

# Check for duplicates
duplicates = data.duplicated().sum()
print("Number of duplicate rows:", duplicates)

data.head()

# Check for missing values
missing_values = data.isnull().sum()
print("Missing values per column:")
print(missing_values)

# Create a new column for the month and year of each contact date
data['YearMonth'] = data['ContactDate'].dt.to_period('M')

# Group the data by the new YearMonth column
monthly_segments = data.groupby('YearMonth')

# Display the size of each segment
segment_sizes = monthly_segments.size()
print("\nSegment sizes (first few):")
print(segment_sizes.head())

# Plot top 5 question categories over time
grouped_data = data.groupby(['YearMonth', 'QuestionTopic_EN']).size().unstack(fill_value=0)
top_categories = grouped_data.sum().nlargest(5).index

plt.figure(figsize=(12, 6))
for category in top_categories:
    plt.plot(grouped_data.index.astype(str), grouped_data[category], label=category, marker='o')

plt.title('Top 5 Question Categories Over Time')
plt.xlabel('Month')
plt.ylabel('Number of Questions')
plt.legend(title='Question Category', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.xticks(rotation=45)
plt.tight_layout()
plt.grid(False)

# Save the plot as an EPS file
plt.savefig('SVG/top_5_categories_over_time.eps', format='eps', dpi=300, bbox_inches='tight')

plt.show()

# Clear the current figure
plt.clf()

# Step 2: Detect drift using CUSUM
def add_drift_column(data):
    initial_mean = data['QuestionThemeID'].mean()
    initial_std = data['QuestionThemeID'].std()
    k = initial_std / 2
    h = 5 * initial_std

    cusum_list = []
    previous_cusum = 0
    for xt in data['QuestionThemeID']:
        current_cusum = max(0, previous_cusum + (xt - initial_mean - k))
        cusum_list.append(current_cusum)
        previous_cusum = current_cusum

    data['CUSUM'] = cusum_list
    data['Drift'] = data['CUSUM'] > h
    return data, h  # Return both the modified dataframe and the threshold value

# Apply the function and unpack the returned values
data, h = add_drift_column(data)

# Now plot the CUSUM chart
plt.figure(figsize=(14, 7))
plt.plot(data['ContactDate'], data['CUSUM'], label='CUSUM', color='blue')
plt.axhline(y=h, color='red', linestyle='--', label='Threshold (h)')
drift_points = data[data['Drift']]
plt.scatter(drift_points['ContactDate'], drift_points['CUSUM'], color='red', label='Potential Drift Points')
plt.xlabel('Date')
plt.ylabel('CUSUM')
plt.title('CUSUM Analysis for Drift Detection')
plt.legend()
plt.grid(False)

# Save the plot as an EPS file
plt.savefig('SVG/CUSUM Analysis for Drift Detection.eps', format='eps', dpi=300, bbox_inches='tight')
plt.show()
# Clear the current figure
plt.clf()

# Step 3: Perform clustering
features = data[['QuestionThemeID', 'QuestionSubthemeID', 'QuestionTopicID']]
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# Determine the optimal number of clusters using the elbow method
sse = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(features_scaled)
    sse.append(kmeans.inertia_)

# Plot the SSE values for the elbow method
plt.figure(figsize=(10, 6))
plt.plot(range(1, 11), sse, marker='o')
plt.xlabel('Number of Clusters')
plt.ylabel('Sum of Squared Errors (SSE)')
plt.title('Elbow Method for Optimal Number of Clusters')
plt.grid(False)

# Save the plot as an EPS file
plt.savefig('SVG/Elbow Method for Optimal Number of Clusters.eps', format='eps', dpi=300, bbox_inches='tight')
plt.show()
# Clear the current figure
plt.clf()

# Apply K-means clustering with the optimal number of clusters
optimal_clusters = 4  # Change this based on the elbow plot
kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)
data['Cluster'] = kmeans.fit_predict(features_scaled)

# Visualize cluster relationships with other features
plt.figure(figsize=(14, 8))
sns.pairplot(data=data, hue='Cluster', vars=['QuestionThemeID', 'QuestionSubthemeID', 'QuestionTopicID'])

# Save the plot as an EPS file
plt.savefig('SVG/PairPlot.eps', format='eps', dpi=300, bbox_inches='tight')
plt.show()
# Clear the current figure
plt.clf()

# Visualize cluster distribution over time
plt.figure(figsize=(12, 6))
cluster_counts = data.groupby(['YearMonth', 'Cluster']).size().unstack(fill_value=0)
cluster_counts.plot(kind='bar', stacked=True)
plt.title('Cluster Distribution Over Time')
plt.xlabel('Month')
plt.ylabel('Count')
plt.legend(title='Cluster', loc='center left', bbox_to_anchor=(1, 0.5))
plt.xticks(rotation=45)
plt.tight_layout()

# Save the plot as an EPS file
plt.savefig('SVG/Cluster Distribution Over Time.eps', format='eps', dpi=300, bbox_inches='tight')
plt.show()
# Clear the current figure
plt.clf()

# Analyze cluster changes over time
plt.figure(figsize=(14, 8))
cluster_changes = data.groupby(['YearMonth', 'Cluster']).size().unstack(fill_value=0)
cluster_changes.plot(kind='line', marker='o')
plt.title('Cluster Changes Over Time')
plt.xlabel('Year-Month')
plt.ylabel('Cluster Size')
plt.legend(labels=cluster_changes.columns, title='Clusters')

# Save the plot as an EPS file
plt.savefig('SVG/Cluster Changes Over Time.eps', format='eps', dpi=300, bbox_inches='tight')
plt.show()
# Clear the current figure
plt.clf()

import os

# Step 4: Process Mining Analysis
# Convert to event log
df = dataframe_utils.convert_timestamp_columns_in_df(data)
df['case:concept:name'] = df['CustomerID']
df['concept:name'] = df['QuestionTheme_EN']
df['time:timestamp'] = df['ContactDate']
event_log = log_converter.apply(df)

# Discover DFG
dfg = dfg_discovery.apply(event_log)

# Visualize DFG for display
display_parameters = {dfg_visualization.Variants.FREQUENCY.value.Parameters.FORMAT: "png"}
gviz_display = dfg_visualization.apply(dfg, log=event_log, variant=dfg_visualization.Variants.FREQUENCY, parameters=display_parameters)

# Display the graph
dfg_visualization.view(gviz_display)

# Visualize DFG for EPS export
export_parameters = {dfg_visualization.Variants.FREQUENCY.value.Parameters.FORMAT: "dot"}
gviz_export = dfg_visualization.apply(dfg, log=event_log, variant=dfg_visualization.Variants.FREQUENCY, parameters=export_parameters)

# Save as DOT file
dot_file = "SVG/dfg_graph.dot"
dfg_visualization.save(gviz_export, dot_file)

# Convert DOT to EPS using Graphviz
eps_file = "SVG/dfg_graph.eps"
os.system(f"dot -Teps {dot_file} -o {eps_file}")

print(f"DFG saved as EPS: {eps_file}")

# Optional: Remove the intermediate DOT file
os.remove(dot_file)

# Filter logs based on timestamp
start_date_pre_drift = '2015-07-01'
end_date_pre_drift = '2015-08-31'
pre_drift_event_log = timestamp_filter.filter_traces_contained(event_log, pd.Timestamp(start_date_pre_drift), pd.Timestamp(end_date_pre_drift))

start_date_post_drift = '2015-09-01'
end_date_post_drift = '2015-10-31'
post_drift_event_log = timestamp_filter.filter_traces_contained(event_log, pd.Timestamp(start_date_post_drift), pd.Timestamp(end_date_post_drift))

# Discover process models
net_pre, initial_marking_pre, final_marking_pre = alpha_miner.apply(pre_drift_event_log)
net_post, initial_marking_post, final_marking_post = alpha_miner.apply(post_drift_event_log)

# Function to visualize and save process model
def visualize_and_save_model(net, initial_marking, final_marking, filename_prefix):
    # Visualize for display
    gviz_display = pn_visualizer.apply(net, initial_marking, final_marking, parameters={"format": "png"})
    pn_visualizer.view(gviz_display)

    # Visualize for EPS export
    gviz_export = pn_visualizer.apply(net, initial_marking, final_marking, parameters={"format": "dot"})

    # Save as DOT file
    dot_file = f"{filename_prefix}_model.dot"
    pn_visualizer.save(gviz_export, dot_file)

    # Convert DOT to EPS using Graphviz
    eps_file = f"{filename_prefix}_model.eps"
    os.system(f"dot -Teps {dot_file} -o {eps_file}")

    print(f"Process model saved as EPS: {eps_file}")

    # Optional: Remove the intermediate DOT file
    os.remove(dot_file)

# Visualize and save pre-drift model
visualize_and_save_model(net_pre, initial_marking_pre, final_marking_pre, "pre_drift")

# Visualize and save post-drift model
visualize_and_save_model(net_post, initial_marking_post, final_marking_post, "post_drift")

# Calculate replay fitness
replay_result_pre = token_replay.apply(pre_drift_event_log, net_pre, initial_marking_pre, final_marking_pre)
pre_drift_fitness = replay_fitness.evaluate(replay_result_pre)

replay_result_post = token_replay.apply(post_drift_event_log, net_post, initial_marking_post, final_marking_post)
post_drift_fitness = replay_fitness.evaluate(replay_result_post)

print("Pre-drift fitness:", pre_drift_fitness)
print("Post-drift fitness:", post_drift_fitness)

# Calculate transition probabilities
pre_drift_ts = heuristics_miner.apply_heu(pre_drift_event_log)
post_drift_ts = heuristics_miner.apply_heu(post_drift_event_log)

# Function to visualize and save transition probabilities
def visualize_and_save_ts(ts, filename_prefix):
    # Visualize for display
    gviz_display = hn_visualizer.apply(ts, parameters={"format": "png"})
    hn_visualizer.view(gviz_display)

    # Visualize for EPS export
    gviz_export = hn_visualizer.apply(ts, parameters={"format": "dot"})

    # Save as DOT file
    dot_file = f"{filename_prefix}_ts.dot"
    hn_visualizer.save(gviz_export, dot_file)

    # Convert DOT to EPS using Graphviz
    eps_file = f"{filename_prefix}_ts.eps"
    os.system(f"dot -Teps {dot_file} -o {eps_file}")

    print(f"Transition probabilities saved as EPS: {eps_file}")

    # Optional: Remove the intermediate DOT file
    os.remove(dot_file)

# Visualize and save pre-drift transition probabilities
visualize_and_save_ts(pre_drift_ts, "pre_drift")

# Visualize and save post-drift transition probabilities
visualize_and_save_ts(post_drift_ts, "post_drift")

# ROC Analysis for Drift Detection Methods
def calculate_cusum_roc(data):
    y_true = data['Drift']
    y_scores = data['CUSUM']
    fpr, tpr, _ = roc_curve(y_true, y_scores)
    roc_auc = auc(fpr, tpr)
    return fpr, tpr, roc_auc

def calculate_kmeans_roc(data):
    features = data[['QuestionThemeID', 'QuestionSubthemeID', 'QuestionTopicID']]
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)

    kmeans = KMeans(n_clusters=2, random_state=42)
    cluster_labels = kmeans.fit_predict(features_scaled)

    drift_cluster = 1 if kmeans.cluster_centers_[1].mean() > kmeans.cluster_centers_[0].mean() else 0
    y_pred = (cluster_labels == drift_cluster).astype(int)

    y_scores = kmeans.transform(features_scaled)[:, drift_cluster]

    fpr, tpr, _ = roc_curve(data['Drift'], y_scores)
    roc_auc = auc(fpr, tpr)
    return fpr, tpr, roc_auc

def calculate_alpha_miner_roc(data):
    data['UniqueActivities'] = data.groupby('CustomerID')['QuestionTopic_EN'].transform('nunique')

    y_true = data['Drift']
    y_scores = data['UniqueActivities']

    fpr, tpr, _ = roc_curve(y_true, y_scores)
    roc_auc = auc(fpr, tpr)
    return fpr, tpr, roc_auc

# Function to plot, display, and save individual ROC curve
def plot_display_save_roc_curve(fpr, tpr, roc_auc, method_name):
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='blue', lw=2, label=f'{method_name} (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {method_name}')
    plt.legend(loc="lower right")

    # Display the plot
    plt.show()

    # Save as EPS
    eps_file = f"roc_curve_{method_name.lower().replace(' ', '_')}.eps"
    plt.savefig(eps_file, format='eps', dpi=300, bbox_inches='tight')
    plt.close()  # Close the figure to free up memory
    print(f"ROC curve saved as EPS: {eps_file}")
    return eps_file

# Ensure 'data' is defined and contains necessary columns before this point
# Calculate ROC values
fpr_cusum, tpr_cusum, roc_auc_cusum = calculate_cusum_roc(data)
fpr_kmeans, tpr_kmeans, roc_auc_kmeans = calculate_kmeans_roc(data)
fpr_alpha, tpr_alpha, roc_auc_alpha = calculate_alpha_miner_roc(data)

# Plot, display, and save separate ROC curves
eps_files = []
eps_files.append(plot_display_save_roc_curve(fpr_cusum, tpr_cusum, roc_auc_cusum, "CUSUM"))
eps_files.append(plot_display_save_roc_curve(fpr_kmeans, tpr_kmeans, roc_auc_kmeans, "K-means"))
eps_files.append(plot_display_save_roc_curve(fpr_alpha, tpr_alpha, roc_auc_alpha, "Alpha Miner Proxy"))

print("\nAll ROC curves have been displayed and saved as EPS files.")
print("To view the saved files, you may need to use an EPS viewer or convert them to another format.")
print("Saved EPS files:")
for file in eps_files:
    print(f"- {file}")

